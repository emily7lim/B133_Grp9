{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495ad2df-b1de-4949-a493-7c51a3797437",
   "metadata": {},
   "source": [
    "# Notebook 3.3: Machine Learning - Bert and Other Features Classification Model\n",
    "\n",
    "After establishing the baseline model in 3.2, we sought to create a model that would leverage both lyrics and additional data. \n",
    "\n",
    "In this notebook, we will utilzed the modified **feature data** as well as **lyrics**, which both show relationship with the popularity of a song, to perform classification by training  our BERT model. Our new model, hence, will **concatenates** the output from Bert with other features, enabling a more comprehensive prediction of song popularity.\n",
    "\n",
    "## Bert and Other Features Classification Model:\n",
    "* [Train-Test Split](#train_test_split)\n",
    "* [Model Construction](#model_con)\n",
    "    * [Data Loader](#dataload)\n",
    "    * [Build Model](#build)\n",
    "    * [Train Model](#train)\n",
    "    * [Result Analysis](#ana)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b17dcbe-e824-42f9-b305-d13cdcd884ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, utils\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6902f2-5bc9-4ac4-81bd-fd1a0ffaff1a",
   "metadata": {},
   "source": [
    "### Import pre-trained tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c7818a-0338-44ac-a693-7b2ebdd246e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./distilbert-base-uncased', local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96846bf-eab1-4258-9e9b-ab6fb56d41ac",
   "metadata": {},
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8704fe-ac73-49b6-83e4-945b76bdd027",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>key_11</th>\n",
       "      <th>tag_country</th>\n",
       "      <th>tag_misc</th>\n",
       "      <th>tag_pop</th>\n",
       "      <th>tag_rap</th>\n",
       "      <th>tag_rb</th>\n",
       "      <th>tag_rock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AKING</td>\n",
       "      <td>2015</td>\n",
       "      <td>4.432273e-05</td>\n",
       "      <td>{}</td>\n",
       "      <td>Glorious mistakes are anxiously waiting to be ...</td>\n",
       "      <td>985583</td>\n",
       "      <td>https://open.spotify.com/track/30sr35axWFPOvmi...</td>\n",
       "      <td>0.760040</td>\n",
       "      <td>0.806517</td>\n",
       "      <td>0.144170</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Filip Winther</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.251733e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Intro]\\nDe-de-deluxe\\n\\n[Refräng]\\nJag fuckar...</td>\n",
       "      <td>5097257</td>\n",
       "      <td>https://open.spotify.com/track/4mznGf6tTvHp74y...</td>\n",
       "      <td>0.020681</td>\n",
       "      <td>0.894094</td>\n",
       "      <td>0.141797</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dan Reeder</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.513459e-05</td>\n",
       "      <td>{}</td>\n",
       "      <td>The guy who bathes in the pond at the park\\nTh...</td>\n",
       "      <td>3407076</td>\n",
       "      <td>https://open.spotify.com/track/1UbSSyqIVEkooKe...</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.554990</td>\n",
       "      <td>0.044422</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noa Azazel</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.251733e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Pre-Chorus]\\nWhen the moon is taking over i'm...</td>\n",
       "      <td>7061926</td>\n",
       "      <td>https://open.spotify.com/track/51F8whLH1Qou7iV...</td>\n",
       "      <td>0.214858</td>\n",
       "      <td>0.419552</td>\n",
       "      <td>0.169140</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>070 Phi</td>\n",
       "      <td>2019</td>\n",
       "      <td>2.031221e-05</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Chorus]\\nAin't no way that you ain't eatin' w...</td>\n",
       "      <td>4241387</td>\n",
       "      <td>https://open.spotify.com/track/0mvzUwvyLT1Dm1y...</td>\n",
       "      <td>0.367469</td>\n",
       "      <td>0.695519</td>\n",
       "      <td>0.146753</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9529</th>\n",
       "      <td>mounika yadav</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.257423e-05</td>\n",
       "      <td>{\"Allu Arjun\",\"Rashmika Mandanna\"}</td>\n",
       "      <td>నువ్ అమ్మీ అమ్మీ అంటాంటే నీ పెళ్ళాన్నైపోయినట్ట...</td>\n",
       "      <td>7552375</td>\n",
       "      <td>https://open.spotify.com/track/4ZUxhQNRCzlh6al...</td>\n",
       "      <td>0.360441</td>\n",
       "      <td>0.821792</td>\n",
       "      <td>0.161581</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9530</th>\n",
       "      <td>d-metal stars</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.706909e-07</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Verse 1]\\nThe seaweed is always greener\\nIn s...</td>\n",
       "      <td>7558599</td>\n",
       "      <td>https://open.spotify.com/track/0F8nLktPi0SgOAm...</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.542770</td>\n",
       "      <td>0.154411</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9531</th>\n",
       "      <td>grupo firme</td>\n",
       "      <td>2021</td>\n",
       "      <td>2.048290e-06</td>\n",
       "      <td>{Maluma}</td>\n",
       "      <td>Dejen de meterse ya, en donde no les importa\\n...</td>\n",
       "      <td>7728445</td>\n",
       "      <td>https://open.spotify.com/track/5BE9B2FiFWBbBdo...</td>\n",
       "      <td>0.137549</td>\n",
       "      <td>0.719959</td>\n",
       "      <td>0.142190</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9532</th>\n",
       "      <td>hensonn</td>\n",
       "      <td>2021</td>\n",
       "      <td>7.567295e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Instrumental]</td>\n",
       "      <td>7814578</td>\n",
       "      <td>https://open.spotify.com/track/6nqdgUTiWt4JbAB...</td>\n",
       "      <td>0.146585</td>\n",
       "      <td>0.626273</td>\n",
       "      <td>0.122640</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9533</th>\n",
       "      <td>ndarboy genk</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.593115e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Intro]\\nMendung tanpo udan\\nKetemu lan kelang...</td>\n",
       "      <td>7822659</td>\n",
       "      <td>https://open.spotify.com/track/0Z54rUZ81Vn0qph...</td>\n",
       "      <td>0.332328</td>\n",
       "      <td>0.613035</td>\n",
       "      <td>0.221762</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9534 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist  year         views                            features  \\\n",
       "0             AKING  2015  4.432273e-05                                  {}   \n",
       "1     Filip Winther  2020  1.251733e-06                                  {}   \n",
       "2        Dan Reeder  2018  1.513459e-05                                  {}   \n",
       "3        Noa Azazel  2021  1.251733e-06                                  {}   \n",
       "4           070 Phi  2019  2.031221e-05                                  {}   \n",
       "...             ...   ...           ...                                 ...   \n",
       "9529  mounika yadav  2021  1.257423e-05  {\"Allu Arjun\",\"Rashmika Mandanna\"}   \n",
       "9530  d-metal stars  2016  1.706909e-07                                  {}   \n",
       "9531    grupo firme  2021  2.048290e-06                            {Maluma}   \n",
       "9532        hensonn  2021  7.567295e-06                                  {}   \n",
       "9533   ndarboy genk  2022  1.593115e-06                                  {}   \n",
       "\n",
       "                                                 lyrics       id  \\\n",
       "0     Glorious mistakes are anxiously waiting to be ...   985583   \n",
       "1     [Intro]\\nDe-de-deluxe\\n\\n[Refräng]\\nJag fuckar...  5097257   \n",
       "2     The guy who bathes in the pond at the park\\nTh...  3407076   \n",
       "3     [Pre-Chorus]\\nWhen the moon is taking over i'm...  7061926   \n",
       "4     [Chorus]\\nAin't no way that you ain't eatin' w...  4241387   \n",
       "...                                                 ...      ...   \n",
       "9529  నువ్ అమ్మీ అమ్మీ అంటాంటే నీ పెళ్ళాన్నైపోయినట్ట...  7552375   \n",
       "9530  [Verse 1]\\nThe seaweed is always greener\\nIn s...  7558599   \n",
       "9531  Dejen de meterse ya, en donde no les importa\\n...  7728445   \n",
       "9532                                     [Instrumental]  7814578   \n",
       "9533  [Intro]\\nMendung tanpo udan\\nKetemu lan kelang...  7822659   \n",
       "\n",
       "                                                    url  acousticness  \\\n",
       "0     https://open.spotify.com/track/30sr35axWFPOvmi...      0.760040   \n",
       "1     https://open.spotify.com/track/4mznGf6tTvHp74y...      0.020681   \n",
       "2     https://open.spotify.com/track/1UbSSyqIVEkooKe...      0.993976   \n",
       "3     https://open.spotify.com/track/51F8whLH1Qou7iV...      0.214858   \n",
       "4     https://open.spotify.com/track/0mvzUwvyLT1Dm1y...      0.367469   \n",
       "...                                                 ...           ...   \n",
       "9529  https://open.spotify.com/track/4ZUxhQNRCzlh6al...      0.360441   \n",
       "9530  https://open.spotify.com/track/0F8nLktPi0SgOAm...      0.000092   \n",
       "9531  https://open.spotify.com/track/5BE9B2FiFWBbBdo...      0.137549   \n",
       "9532  https://open.spotify.com/track/6nqdgUTiWt4JbAB...      0.146585   \n",
       "9533  https://open.spotify.com/track/0Z54rUZ81Vn0qph...      0.332328   \n",
       "\n",
       "      danceability  duration_ms  ...  key_8  key_9  key_10  key_11  \\\n",
       "0         0.806517     0.144170  ...      0      0       0       0   \n",
       "1         0.894094     0.141797  ...      0      0       0       1   \n",
       "2         0.554990     0.044422  ...      0      0       0       0   \n",
       "3         0.419552     0.169140  ...      0      0       0       0   \n",
       "4         0.695519     0.146753  ...      0      0       0       0   \n",
       "...            ...          ...  ...    ...    ...     ...     ...   \n",
       "9529      0.821792     0.161581  ...      0      0       0       0   \n",
       "9530      0.542770     0.154411  ...      1      0       0       0   \n",
       "9531      0.719959     0.142190  ...      0      0       0       0   \n",
       "9532      0.626273     0.122640  ...      0      0       0       0   \n",
       "9533      0.613035     0.221762  ...      0      0       0       0   \n",
       "\n",
       "      tag_country  tag_misc  tag_pop  tag_rap  tag_rb tag_rock  \n",
       "0               0         0        1        0       0        0  \n",
       "1               0         0        0        1       0        0  \n",
       "2               0         0        1        0       0        0  \n",
       "3               0         0        1        0       0        0  \n",
       "4               0         0        0        1       0        0  \n",
       "...           ...       ...      ...      ...     ...      ...  \n",
       "9529            0         0        1        0       0        0  \n",
       "9530            0         0        0        0       0        1  \n",
       "9531            0         0        1        0       0        0  \n",
       "9532            0         0        0        1       0        0  \n",
       "9533            0         0        1        0       0        0  \n",
       "\n",
       "[9534 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./positive_and_negative_one_hot.csv\")\n",
    "df = df.dropna()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d3ada-b959-4029-bc83-cf9204ca21de",
   "metadata": {},
   "source": [
    "## Train-Test Split <a name = \"train_test_split\"> </a>\n",
    "\n",
    "**Train Dataset**: Used to fit the machine learning model.\n",
    "\n",
    "**Test Dataset**: Used to evaluate the fit machine learning model.\n",
    "\n",
    "The objective is to estimate the performance of the machine learning model on new data: data not used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df411ef9-4adb-44c5-9f4b-7d3fbb2e088e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(6666)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=88), \n",
    "                                     [int(.6*len(df)), int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fccac5-999e-4923-9d22-d1e6c50c9628",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Construction <a name = \"model_con\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e628f722-902b-45fe-a67c-3fed84d0f3c0",
   "metadata": {},
   "source": [
    "## Data Loader <a name = \"dataload\"> </a>\n",
    "\n",
    "**Get Batch Data**: Iterate through the dataset with batch size = 32.\n",
    "\n",
    "Code for processing data samples can get messy and hard to maintain, thus the dataset code can be used to decouple data from our model training code for better readability and modularity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6776c6-ee56-4520-ac00-a1625a1d09d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.ys = df['if_popular'].to_numpy()\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['lyrics']]\n",
    "        self.features = df[['key_0','key_1','key_2','key_3','key_4','key_5','key_6','key_7','key_8','key_9','key_10','key_11','tag_country','tag_misc','tag_pop','tag_rap','tag_rb','tag_rock','year', 'views','acousticness','danceability','duration_ms','energy','instrumentalness','liveness','loudness','speechiness','tempo','valence','popularity']].to_numpy()\n",
    "        self.df = df\n",
    "\n",
    "    def linear(self):\n",
    "        return self.ys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        #print(\"Total len:\", len(self.ys), \" getting:\", idx)\n",
    "        return self.ys[idx]\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def get_batch_freatures(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.features[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        batch_feature = self.get_batch_freatures(idx)\n",
    "\n",
    "        return batch_texts, batch_y, batch_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a57985-0690-4fbd-bb78-86e09c4bc333",
   "metadata": {},
   "source": [
    "## Build Model <a name = \"build\"> </a>\n",
    "\n",
    "\n",
    "**BertModel**: BERT makes use of Transformer, an attention mechanism that learns contextual relations between words in a text.\n",
    "\n",
    "**Drop out layers**: Dropout is a regularization technique to help prevent overfitting. As it randomly drop out nodes during training, tje model could become more generalized. \n",
    "\n",
    "**ReLU**: The rectified linear unit (ReLU) is an activation function commonly used in neural networks. It return 0 if the input value is non-positive.\n",
    "\n",
    "**BatchNorm**: Batch normalization take the outputs from the a hidden layer and normalize them before passing them as the input of the next hidden layer, which can stabilizing the learning process and greatly reducing the number of training epochs.\n",
    "\n",
    "\n",
    "In this classifier, we would like to utilize both lyrics and additional features to predict a song's popularity. The method for combining these two types of data is crucial, which might have direct impact on the model's performance. One approach involves embedding the data within a sentence, such as appending \"The song's popularity is 6\" to the end of a sentence. Alternatively, we could concatenate the data within the Multilayer Perceptron (MLP) layers. In this model, we have opted for the latter method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ae69d1-1bc5-4e35-add5-f0495eabd03b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('./distilbert-base-uncased', local_files_only=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(768, 64)\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.linear3 = nn.Linear(63, 16)\n",
    "        self.layer_out = nn.Linear(16, 1) \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask, features):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output1 = self.relu(self.linear1(dropout_output))\n",
    "        linear_output1 = self.batchnorm1(linear_output1)\n",
    "        linear_output2 = self.relu(self.linear2(linear_output1))\n",
    "        linear_output2 = self.batchnorm2(linear_output2)\n",
    "        linear_output2 = self.dropout(linear_output2)\n",
    "        linear_output3 = self.relu(self.linear3(torch.cat((linear_output2, features), dim=1)))\n",
    "        linear_output3 = self.batchnorm3(linear_output3)\n",
    "        linear_output3 = self.dropout(linear_output3)\n",
    "        final_layer = self.layer_out(linear_output3)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ce7996-12c0-4eb0-86ed-bdc15cad0176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    accuracy = accuracy_score(y_true, y_prob > 0.5)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688efc1-a96f-4e4a-8b4d-2582791b945e",
   "metadata": {},
   "source": [
    "## Train Model <a name = \"train\"> </a>\n",
    "\n",
    "**Batch Size**: Batch Size is the number of training examples used in one iteration. A large batch size might hinder the quality of the model and its ability to generalize, as the model might converge to sharp minimizers of the training function. However, small batch size would be too noisy for the model to convergence fast.\n",
    "\n",
    "**Adam Optimizer**: Adam is a stochastic gradient descent (SGD) method that inherit the features of two popular adaptive learning rate methods: AdaGrad and RMSProp. It requires minimal tuning of hyperparameters, is known to for a faster convergence and better performance than traditional SGD and other adaptive learning rate methods.\n",
    "\n",
    "**BCEWithLogitsLoss**: BCEWithLogitsLoss is a loss function that combines the Binary Cross Entropy (BCE) Loss with a sigmoid activation function together. It is designed for binary classification problems, where the goal is to distinguish between two classes that are represented by labels 0 and 1 respectively.\n",
    "\n",
    "\n",
    "**SummaryWriter**: SummaryWriter is a library that allows user to log various types of data (Time Series and summary statistic) for visualization in TensorBoard. TensorBoard is a web-based visualization tool developed by Google as part of the TensorFlow ecosystem, but it can also be used with other frameworks like PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352f592-7c57-4e2f-ab36-90631212d2af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ./distilbert-base-uncased were not used when initializing BertModel: ['distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./distilbert-base-uncased and are newly initialized: ['encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-04-21 21:21:52.397419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | \n",
      "Train BCELoss:  0.581                 | Val BCELoss:  0.526\n",
      "Train Acc:  0.628                 | Val Acc:  0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | \n",
      "Train BCELoss:  0.512                 | Val BCELoss:  0.507\n",
      "Train Acc:  0.716                 | Val Acc:  0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | \n",
      "Train BCELoss:  0.482                 | Val BCELoss:  0.512\n",
      "Train Acc:  0.762                 | Val Acc:  0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | \n",
      "Train BCELoss:  0.443                 | Val BCELoss:  0.484\n",
      "Train Acc:  0.817                 | Val Acc:  0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | \n",
      "Train BCELoss:  0.409                 | Val BCELoss:  0.472\n",
      "Train Acc:  0.856                 | Val Acc:  0.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | \n",
      "Train BCELoss:  0.383                 | Val BCELoss:  0.464\n",
      "Train Acc:  0.890                 | Val Acc:  0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | \n",
      "Train BCELoss:  0.361                 | Val BCELoss:  0.467\n",
      "Train Acc:  0.913                 | Val Acc:  0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | \n",
      "Train BCELoss:  0.331                 | Val BCELoss:  0.467\n",
      "Train Acc:  0.946                 | Val Acc:  0.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | \n",
      "Train BCELoss:  0.312                 | Val BCELoss:  0.468\n",
      "Train Acc:  0.956                 | Val Acc:  0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | \n",
      "Train BCELoss:  0.301                 | Val BCELoss:  0.471\n",
      "Train Acc:  0.966                 | Val Acc:  0.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | \n",
      "Train BCELoss:  0.288                 | Val BCELoss:  0.462\n",
      "Train Acc:  0.972                 | Val Acc:  0.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | \n",
      "Train BCELoss:  0.288                 | Val BCELoss:  0.488\n",
      "Train Acc:  0.962                 | Val Acc:  0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | \n",
      "Train BCELoss:  0.277                 | Val BCELoss:  0.491\n",
      "Train Acc:  0.966                 | Val Acc:  0.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | \n",
      "Train BCELoss:  0.265                 | Val BCELoss:  0.457\n",
      "Train Acc:  0.976                 | Val Acc:  0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | \n",
      "Train BCELoss:  0.252                 | Val BCELoss:  0.460\n",
      "Train Acc:  0.977                 | Val Acc:  0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 16 | \n",
      "Train BCELoss:  0.247                 | Val BCELoss:  0.460\n",
      "Train Acc:  0.977                 | Val Acc:  0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 17 | \n",
      "Train BCELoss:  0.235                 | Val BCELoss:  0.489\n",
      "Train Acc:  0.986                 | Val Acc:  0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 18 | \n",
      "Train BCELoss:  0.227                 | Val BCELoss:  0.460\n",
      "Train Acc:  0.985                 | Val Acc:  0.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 19 | \n",
      "Train BCELoss:  0.242                 | Val BCELoss:  0.467\n",
      "Train Acc:  0.968                 | Val Acc:  0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 20 | \n",
      "Train BCELoss:  0.219                 | Val BCELoss:  0.481\n",
      "Train Acc:  0.983                 | Val Acc:  0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 21 | \n",
      "Train BCELoss:  0.216                 | Val BCELoss:  0.462\n",
      "Train Acc:  0.980                 | Val Acc:  0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 22 | \n",
      "Train BCELoss:  0.203                 | Val BCELoss:  0.472\n",
      "Train Acc:  0.986                 | Val Acc:  0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 23 | \n",
      "Train BCELoss:  0.199                 | Val BCELoss:  0.496\n",
      "Train Acc:  0.985                 | Val Acc:  0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 24 | \n",
      "Train BCELoss:  0.192                 | Val BCELoss:  0.479\n",
      "Train Acc:  0.987                 | Val Acc:  0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 25 | \n",
      "Train BCELoss:  0.187                 | Val BCELoss:  0.476\n",
      "Train Acc:  0.987                 | Val Acc:  0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 26 | \n",
      "Train BCELoss:  0.197                 | Val BCELoss:  0.571\n",
      "Train Acc:  0.977                 | Val Acc:  0.749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 27 | \n",
      "Train BCELoss:  0.189                 | Val BCELoss:  0.490\n",
      "Train Acc:  0.976                 | Val Acc:  0.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 28 | \n",
      "Train BCELoss:  0.179                 | Val BCELoss:  0.475\n",
      "Train Acc:  0.982                 | Val Acc:  0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 29 | \n",
      "Train BCELoss:  0.170                 | Val BCELoss:  0.545\n",
      "Train Acc:  0.985                 | Val Acc:  0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 30 | \n",
      "Train BCELoss:  0.164                 | Val BCELoss:  0.482\n",
      "Train Acc:  0.987                 | Val Acc:  0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 31 | \n",
      "Train BCELoss:  0.160                 | Val BCELoss:  0.479\n",
      "Train Acc:  0.986                 | Val Acc:  0.822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 32 | \n",
      "Train BCELoss:  0.155                 | Val BCELoss:  0.475\n",
      "Train Acc:  0.987                 | Val Acc:  0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 33 | \n",
      "Train BCELoss:  0.147                 | Val BCELoss:  0.481\n",
      "Train Acc:  0.989                 | Val Acc:  0.822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:54<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 34 | \n",
      "Train BCELoss:  0.148                 | Val BCELoss:  0.493\n",
      "Train Acc:  0.989                 | Val Acc:  0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:54<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 35 | \n",
      "Train BCELoss:  0.140                 | Val BCELoss:  0.499\n",
      "Train Acc:  0.989                 | Val Acc:  0.822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:54<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 36 | \n",
      "Train BCELoss:  0.140                 | Val BCELoss:  0.497\n",
      "Train Acc:  0.989                 | Val Acc:  0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 37 | \n",
      "Train BCELoss:  0.133                 | Val BCELoss:  0.499\n",
      "Train Acc:  0.989                 | Val Acc:  0.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:54<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 38 | \n",
      "Train BCELoss:  0.133                 | Val BCELoss:  0.496\n",
      "Train Acc:  0.989                 | Val Acc:  0.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 39 | \n",
      "Train BCELoss:  0.128                 | Val BCELoss:  0.584\n",
      "Train Acc:  0.990                 | Val Acc:  0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:55<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 40 | \n",
      "Train BCELoss:  0.688                 | Val BCELoss:  0.761\n",
      "Train Acc:  0.624                 | Val Acc:  0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:54<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 41 | \n",
      "Train BCELoss:  0.843                 | Val BCELoss:  0.946\n",
      "Train Acc:  0.544                 | Val Acc:  0.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:54<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 42 | \n",
      "Train BCELoss:  0.858                 | Val BCELoss:  0.637\n",
      "Train Acc:  0.534                 | Val Acc:  0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:54<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 43 | \n",
      "Train BCELoss:  0.520                 | Val BCELoss:  0.595\n",
      "Train Acc:  0.763                 | Val Acc:  0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:54<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 44 | \n",
      "Train BCELoss:  0.500                 | Val BCELoss:  0.592\n",
      "Train Acc:  0.781                 | Val Acc:  0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:54<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 45 | \n",
      "Train BCELoss:  0.485                 | Val BCELoss:  0.566\n",
      "Train Acc:  0.784                 | Val Acc:  0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:54<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 46 | \n",
      "Train BCELoss:  0.504                 | Val BCELoss:  0.508\n",
      "Train Acc:  0.750                 | Val Acc:  0.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|████████████████████████████████████████████████████████████████████████████████████████████████▋      | 168/179 [01:47<00:07,  1.55it/s]"
     ]
    }
   ],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_cnt_train = 0\n",
    "            total_loss_train = 0\n",
    "            train_acc = 0\n",
    "            train_acc_cnt = 0\n",
    "\n",
    "            for train_input, train_label, train_features in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                train_features = train_features.to(torch.float32).to(device)\n",
    "\n",
    "                output = model(input_id, mask, train_features)\n",
    "                \n",
    "                # print(\"Output1: \", output, \" Output2: \", train_label.float().unsqueeze(1), \" loss: \" , criterion(output, train_label.float()))\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.float().unsqueeze(1))\n",
    "                total_loss_train += batch_loss.item()\n",
    "                # if total_cnt_train == 5:\n",
    "                #     print(\"Train LOSS:\", batch_loss.item())\n",
    "                total_cnt_train += 1\n",
    "                \n",
    "                train_acc += get_accuracy(train_label.float().unsqueeze(1).cpu(), output.cpu())\n",
    "                train_acc_cnt += 1\n",
    " \n",
    "    \n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_cnt_val = 0\n",
    "            total_loss_val = 0\n",
    "            acc = 0\n",
    "            acc_cnt = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label, val_features in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                    val_features = val_features.to(torch.float32).to(device)\n",
    "\n",
    "                    output = model(input_id, mask, val_features)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.float().unsqueeze(1))\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    # if total_cnt_val == 5:\n",
    "                    #     print(\"Val LOSS:\", batch_loss.item())\n",
    "                    total_cnt_val += 1\n",
    "                    # print(\"Loss: \", batch_loss, \" Calc: \", sum((output - val_label.float().unsqueeze(1))**2) / batch_size)\n",
    "                    acc += get_accuracy(val_label.float().unsqueeze(1).cpu(), output.cpu())\n",
    "                    acc_cnt += 1\n",
    "\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | \\nTrain BCELoss: {total_loss_train / total_cnt_train: .3f} \\\n",
    "                | Val BCELoss: {total_loss_val / total_cnt_val: .3f}' +\n",
    "                f'\\nTrain Acc: {train_acc / train_acc_cnt: .3f} \\\n",
    "                | Val Acc: {acc / acc_cnt: .3f}'\n",
    "            )\n",
    "                  \n",
    "            writer.add_scalar('Loss/train', total_loss_train / total_cnt_train, epoch_num)\n",
    "            writer.add_scalar('Loss/val', total_loss_val / total_cnt_val, epoch_num)\n",
    "            writer.add_scalar('Acc/train', train_acc / train_acc_cnt, epoch_num)\n",
    "            writer.add_scalar('Acc/val', acc / acc_cnt, epoch_num)\n",
    "            torch.save(model.state_dict(), \"./Bert_classification/BERT-CLASSIFICATION_it\" + str(epoch_num) + \".pt\")\n",
    "\n",
    "\n",
    "EPOCHS = 50\n",
    "model = BertClassifier()\n",
    "LR = 3e-5\n",
    "     \n",
    "writer = SummaryWriter()\n",
    "train(model, df_train, df_val, LR, EPOCHS)\n",
    "writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb242a0-f268-42d4-a080-4730ecda3977",
   "metadata": {},
   "source": [
    "## Result Analysis <a name = \"ana\"> </a>\n",
    "\n",
    "**Accuracy**: As anticipated, this model achieved **higher accuracy** compared to the baseline models, which relied solely on BERT, or traditional approaches utilizing Support Vector Machines (SVM) and other models. This highlights the fact that **both lyrics and additional features** of a song contribute to **determining its popularity**.\n",
    "\n",
    "**Early Stopping**: Early stopping is crucial, as demonstrated by the data above. If the number of iterations becomes excessive, the **test set's loss** starts to **increase due to overfitting**. In our specific case, halting at around the `25th` iteration led to a more reliable and accurate model.\n",
    "\n",
    "![Results33](./3.3Result.png \"Results33\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee04cd9-4ac1-45d2-8fd9-4da32063974f",
   "metadata": {},
   "source": [
    "Next, we would elaborate about the insight driven by our various models in the next notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8541ba-dd5e-42e3-b62d-7e03eed72cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
