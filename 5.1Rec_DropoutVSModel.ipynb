{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5.1: Recommendations - How dropout affect models\n",
    "\n",
    "Dropout is a common regularization method in machine learning, especially for deep learning models. It prevents overfitting, which will result in poor performance on unseen data. Overfitting reduces a model's generalizability and effectiveness with new sample introduced.\n",
    "\n",
    "During training, dropout randomly \"deactivates\" a portion of connections in a neural network. This occurs at every training iteration, with the probability of a neuron being dropped determined by the dropout rate. Consequently, the model learns multiple representations. It then will not depend too much on certain neurons or connections, preventing overfitting.\n",
    "\n",
    "In this notebook, our goal is to remove the dropout layers from the model in Section 3.3 and compare the performance differences to better understand dropout's significance.\n",
    "\n",
    "## How dropout affect models\n",
    "* [Remove dropout layers, and retrain model](#rem)\n",
    "    * [Remove dropout layers](#rem)\n",
    "    * [Retrain the model](#train)\n",
    "* [The result](#vis)\n",
    "    * [Visualization](#vis)\n",
    "    * [Conclusion](#conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hy\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, utils\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./distilbert-base-uncased', local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the definition from huggingface's website https://huggingface.co/distilbert-base-uncased, DistilBERT is a small, fast, cheap and light Transformer model which was **pretrained** on the same corpus in a **self-supervised** fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only, with no humans labelling them in any way.\n",
    "\n",
    "This makes it suitable for our model development to focus on oue lyrics as we are trying to investigate its relationship with the popularity of the song."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>key_11</th>\n",
       "      <th>tag_country</th>\n",
       "      <th>tag_misc</th>\n",
       "      <th>tag_pop</th>\n",
       "      <th>tag_rap</th>\n",
       "      <th>tag_rb</th>\n",
       "      <th>tag_rock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AKING</td>\n",
       "      <td>2015</td>\n",
       "      <td>4.432273e-05</td>\n",
       "      <td>{}</td>\n",
       "      <td>Glorious mistakes are anxiously waiting to be ...</td>\n",
       "      <td>985583</td>\n",
       "      <td>https://open.spotify.com/track/30sr35axWFPOvmi...</td>\n",
       "      <td>0.760040</td>\n",
       "      <td>0.806517</td>\n",
       "      <td>0.144170</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Filip Winther</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.251733e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Intro]\\nDe-de-deluxe\\n\\n[Refräng]\\nJag fuckar...</td>\n",
       "      <td>5097257</td>\n",
       "      <td>https://open.spotify.com/track/4mznGf6tTvHp74y...</td>\n",
       "      <td>0.020681</td>\n",
       "      <td>0.894094</td>\n",
       "      <td>0.141797</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dan Reeder</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.513459e-05</td>\n",
       "      <td>{}</td>\n",
       "      <td>The guy who bathes in the pond at the park\\nTh...</td>\n",
       "      <td>3407076</td>\n",
       "      <td>https://open.spotify.com/track/1UbSSyqIVEkooKe...</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.554990</td>\n",
       "      <td>0.044422</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noa Azazel</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.251733e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Pre-Chorus]\\nWhen the moon is taking over i'm...</td>\n",
       "      <td>7061926</td>\n",
       "      <td>https://open.spotify.com/track/51F8whLH1Qou7iV...</td>\n",
       "      <td>0.214858</td>\n",
       "      <td>0.419552</td>\n",
       "      <td>0.169140</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>070 Phi</td>\n",
       "      <td>2019</td>\n",
       "      <td>2.031221e-05</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Chorus]\\nAin't no way that you ain't eatin' w...</td>\n",
       "      <td>4241387</td>\n",
       "      <td>https://open.spotify.com/track/0mvzUwvyLT1Dm1y...</td>\n",
       "      <td>0.367469</td>\n",
       "      <td>0.695519</td>\n",
       "      <td>0.146753</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9529</th>\n",
       "      <td>mounika yadav</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.257423e-05</td>\n",
       "      <td>{\"Allu Arjun\",\"Rashmika Mandanna\"}</td>\n",
       "      <td>నువ్ అమ్మీ అమ్మీ అంటాంటే నీ పెళ్ళాన్నైపోయినట్ట...</td>\n",
       "      <td>7552375</td>\n",
       "      <td>https://open.spotify.com/track/4ZUxhQNRCzlh6al...</td>\n",
       "      <td>0.360441</td>\n",
       "      <td>0.821792</td>\n",
       "      <td>0.161581</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9530</th>\n",
       "      <td>d-metal stars</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.706909e-07</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Verse 1]\\nThe seaweed is always greener\\nIn s...</td>\n",
       "      <td>7558599</td>\n",
       "      <td>https://open.spotify.com/track/0F8nLktPi0SgOAm...</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.542770</td>\n",
       "      <td>0.154411</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9531</th>\n",
       "      <td>grupo firme</td>\n",
       "      <td>2021</td>\n",
       "      <td>2.048290e-06</td>\n",
       "      <td>{Maluma}</td>\n",
       "      <td>Dejen de meterse ya, en donde no les importa\\n...</td>\n",
       "      <td>7728445</td>\n",
       "      <td>https://open.spotify.com/track/5BE9B2FiFWBbBdo...</td>\n",
       "      <td>0.137549</td>\n",
       "      <td>0.719959</td>\n",
       "      <td>0.142190</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9532</th>\n",
       "      <td>hensonn</td>\n",
       "      <td>2021</td>\n",
       "      <td>7.567295e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Instrumental]</td>\n",
       "      <td>7814578</td>\n",
       "      <td>https://open.spotify.com/track/6nqdgUTiWt4JbAB...</td>\n",
       "      <td>0.146585</td>\n",
       "      <td>0.626273</td>\n",
       "      <td>0.122640</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9533</th>\n",
       "      <td>ndarboy genk</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.593115e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Intro]\\nMendung tanpo udan\\nKetemu lan kelang...</td>\n",
       "      <td>7822659</td>\n",
       "      <td>https://open.spotify.com/track/0Z54rUZ81Vn0qph...</td>\n",
       "      <td>0.332328</td>\n",
       "      <td>0.613035</td>\n",
       "      <td>0.221762</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9532 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist  year         views                            features  \\\n",
       "0             AKING  2015  4.432273e-05                                  {}   \n",
       "1     Filip Winther  2020  1.251733e-06                                  {}   \n",
       "2        Dan Reeder  2018  1.513459e-05                                  {}   \n",
       "3        Noa Azazel  2021  1.251733e-06                                  {}   \n",
       "4           070 Phi  2019  2.031221e-05                                  {}   \n",
       "...             ...   ...           ...                                 ...   \n",
       "9529  mounika yadav  2021  1.257423e-05  {\"Allu Arjun\",\"Rashmika Mandanna\"}   \n",
       "9530  d-metal stars  2016  1.706909e-07                                  {}   \n",
       "9531    grupo firme  2021  2.048290e-06                            {Maluma}   \n",
       "9532        hensonn  2021  7.567295e-06                                  {}   \n",
       "9533   ndarboy genk  2022  1.593115e-06                                  {}   \n",
       "\n",
       "                                                 lyrics       id  \\\n",
       "0     Glorious mistakes are anxiously waiting to be ...   985583   \n",
       "1     [Intro]\\nDe-de-deluxe\\n\\n[Refräng]\\nJag fuckar...  5097257   \n",
       "2     The guy who bathes in the pond at the park\\nTh...  3407076   \n",
       "3     [Pre-Chorus]\\nWhen the moon is taking over i'm...  7061926   \n",
       "4     [Chorus]\\nAin't no way that you ain't eatin' w...  4241387   \n",
       "...                                                 ...      ...   \n",
       "9529  నువ్ అమ్మీ అమ్మీ అంటాంటే నీ పెళ్ళాన్నైపోయినట్ట...  7552375   \n",
       "9530  [Verse 1]\\nThe seaweed is always greener\\nIn s...  7558599   \n",
       "9531  Dejen de meterse ya, en donde no les importa\\n...  7728445   \n",
       "9532                                     [Instrumental]  7814578   \n",
       "9533  [Intro]\\nMendung tanpo udan\\nKetemu lan kelang...  7822659   \n",
       "\n",
       "                                                    url  acousticness  \\\n",
       "0     https://open.spotify.com/track/30sr35axWFPOvmi...      0.760040   \n",
       "1     https://open.spotify.com/track/4mznGf6tTvHp74y...      0.020681   \n",
       "2     https://open.spotify.com/track/1UbSSyqIVEkooKe...      0.993976   \n",
       "3     https://open.spotify.com/track/51F8whLH1Qou7iV...      0.214858   \n",
       "4     https://open.spotify.com/track/0mvzUwvyLT1Dm1y...      0.367469   \n",
       "...                                                 ...           ...   \n",
       "9529  https://open.spotify.com/track/4ZUxhQNRCzlh6al...      0.360441   \n",
       "9530  https://open.spotify.com/track/0F8nLktPi0SgOAm...      0.000092   \n",
       "9531  https://open.spotify.com/track/5BE9B2FiFWBbBdo...      0.137549   \n",
       "9532  https://open.spotify.com/track/6nqdgUTiWt4JbAB...      0.146585   \n",
       "9533  https://open.spotify.com/track/0Z54rUZ81Vn0qph...      0.332328   \n",
       "\n",
       "      danceability  duration_ms  ...  key_8  key_9  key_10  key_11  \\\n",
       "0         0.806517     0.144170  ...      0      0       0       0   \n",
       "1         0.894094     0.141797  ...      0      0       0       1   \n",
       "2         0.554990     0.044422  ...      0      0       0       0   \n",
       "3         0.419552     0.169140  ...      0      0       0       0   \n",
       "4         0.695519     0.146753  ...      0      0       0       0   \n",
       "...            ...          ...  ...    ...    ...     ...     ...   \n",
       "9529      0.821792     0.161581  ...      0      0       0       0   \n",
       "9530      0.542770     0.154411  ...      1      0       0       0   \n",
       "9531      0.719959     0.142190  ...      0      0       0       0   \n",
       "9532      0.626273     0.122640  ...      0      0       0       0   \n",
       "9533      0.613035     0.221762  ...      0      0       0       0   \n",
       "\n",
       "      tag_country  tag_misc  tag_pop  tag_rap  tag_rb tag_rock  \n",
       "0               0         0        1        0       0        0  \n",
       "1               0         0        0        1       0        0  \n",
       "2               0         0        1        0       0        0  \n",
       "3               0         0        1        0       0        0  \n",
       "4               0         0        0        1       0        0  \n",
       "...           ...       ...      ...      ...     ...      ...  \n",
       "9529            0         0        1        0       0        0  \n",
       "9530            0         0        0        0       0        1  \n",
       "9531            0         0        1        0       0        0  \n",
       "9532            0         0        0        1       0        0  \n",
       "9533            0         0        1        0       0        0  \n",
       "\n",
       "[9532 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./positive_and_negative_one_hot.csv\")\n",
    "df = df.dropna()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6666)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=88), \n",
    "                                     [int(.6*len(df)), int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.ys = df['if_popular'].to_numpy()\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['lyrics']]\n",
    "        self.features = df[['key_0','key_1','key_2','key_3','key_4','key_5','key_6','key_7','key_8','key_9','key_10','key_11','tag_country','tag_misc','tag_pop','tag_rap','tag_rb','tag_rock','year', 'views','acousticness','danceability','duration_ms','energy','instrumentalness','liveness','loudness','speechiness','tempo','valence','popularity']].to_numpy()\n",
    "        self.df = df\n",
    "\n",
    "    def linear(self):\n",
    "        return self.ys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        #print(\"Total len:\", len(self.ys), \" getting:\", idx)\n",
    "        return self.ys[idx]\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "    \n",
    "    def get_batch_freatures(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.features[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        batch_feature = self.get_batch_freatures(idx)\n",
    "\n",
    "        return batch_texts, batch_y, batch_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Remove dropout layers <a name = \"rem\"> </a>\n",
    "\n",
    "The model below is identical to the model in 3.3, however, all dropout layers are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('./distilbert-base-uncased', local_files_only=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(768, 64)\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.linear3 = nn.Linear(63, 16)\n",
    "        self.layer_out = nn.Linear(16, 1) \n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask, features):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        # dropout_output = self.dropout(pooled_output)\n",
    "        linear_output1 = self.relu(self.linear1(pooled_output))\n",
    "        linear_output1 = self.batchnorm1(linear_output1)\n",
    "        linear_output2 = self.relu(self.linear2(linear_output1))\n",
    "        linear_output2 = self.batchnorm2(linear_output2)\n",
    "        # linear_output2 = self.dropout(linear_output2)\n",
    "        linear_output3 = self.relu(self.linear3(torch.cat((linear_output2, features), dim=1)))\n",
    "        linear_output3 = self.batchnorm3(linear_output3)\n",
    "        # linear_output3 = self.dropout(linear_output3)\n",
    "        final_layer = self.layer_out(linear_output3)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    accuracy = accuracy_score(y_true, y_prob > 0.5)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model <a name = \"train\"> </a>\n",
    "\n",
    "Dropout is a regularization technique used in neural networks to reduce overfitting. In this model, we removed dropout and tested its effect.\n",
    "\n",
    "The model below is mostly identical to the training process in 3.3 with no dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ./distilbert-base-uncased were not used when initializing BertModel: ['distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./distilbert-base-uncased and are newly initialized: ['encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.query.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'pooler.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'pooler.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | \n",
      "Train BCELoss:  0.560                 | Val BCELoss:  0.539\n",
      "Train Acc:  0.686                 | Val Acc:  0.741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | \n",
      "Train BCELoss:  0.495                 | Val BCELoss:  0.522\n",
      "Train Acc:  0.777                 | Val Acc:  0.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | \n",
      "Train BCELoss:  0.451                 | Val BCELoss:  0.547\n",
      "Train Acc:  0.816                 | Val Acc:  0.730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | \n",
      "Train BCELoss:  0.406                 | Val BCELoss:  0.498\n",
      "Train Acc:  0.863                 | Val Acc:  0.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | \n",
      "Train BCELoss:  0.362                 | Val BCELoss:  0.487\n",
      "Train Acc:  0.903                 | Val Acc:  0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | \n",
      "Train BCELoss:  0.329                 | Val BCELoss:  0.480\n",
      "Train Acc:  0.939                 | Val Acc:  0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | \n",
      "Train BCELoss:  0.304                 | Val BCELoss:  0.491\n",
      "Train Acc:  0.959                 | Val Acc:  0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | \n",
      "Train BCELoss:  0.274                 | Val BCELoss:  0.482\n",
      "Train Acc:  0.980                 | Val Acc:  0.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | \n",
      "Train BCELoss:  0.256                 | Val BCELoss:  0.478\n",
      "Train Acc:  0.985                 | Val Acc:  0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | \n",
      "Train BCELoss:  0.242                 | Val BCELoss:  0.479\n",
      "Train Acc:  0.989                 | Val Acc:  0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | \n",
      "Train BCELoss:  0.231                 | Val BCELoss:  0.476\n",
      "Train Acc:  0.990                 | Val Acc:  0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | \n",
      "Train BCELoss:  0.219                 | Val BCELoss:  0.475\n",
      "Train Acc:  0.993                 | Val Acc:  0.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | \n",
      "Train BCELoss:  0.207                 | Val BCELoss:  0.491\n",
      "Train Acc:  0.993                 | Val Acc:  0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | \n",
      "Train BCELoss:  0.200                 | Val BCELoss:  0.460\n",
      "Train Acc:  0.992                 | Val Acc:  0.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | \n",
      "Train BCELoss:  0.196                 | Val BCELoss:  0.494\n",
      "Train Acc:  0.990                 | Val Acc:  0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 16 | \n",
      "Train BCELoss:  0.185                 | Val BCELoss:  0.477\n",
      "Train Acc:  0.992                 | Val Acc:  0.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 17 | \n",
      "Train BCELoss:  0.182                 | Val BCELoss:  0.471\n",
      "Train Acc:  0.991                 | Val Acc:  0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 18 | \n",
      "Train BCELoss:  0.192                 | Val BCELoss:  0.479\n",
      "Train Acc:  0.982                 | Val Acc:  0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 19 | \n",
      "Train BCELoss:  0.160                 | Val BCELoss:  0.500\n",
      "Train Acc:  0.994                 | Val Acc:  0.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 20 | \n",
      "Train BCELoss:  0.154                 | Val BCELoss:  0.484\n",
      "Train Acc:  0.995                 | Val Acc:  0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 21 | \n",
      "Train BCELoss:  0.148                 | Val BCELoss:  0.512\n",
      "Train Acc:  0.995                 | Val Acc:  0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 22 | \n",
      "Train BCELoss:  0.141                 | Val BCELoss:  0.490\n",
      "Train Acc:  0.995                 | Val Acc:  0.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 23 | \n",
      "Train BCELoss:  0.137                 | Val BCELoss:  0.499\n",
      "Train Acc:  0.995                 | Val Acc:  0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 24 | \n",
      "Train BCELoss:  0.136                 | Val BCELoss:  0.517\n",
      "Train Acc:  0.994                 | Val Acc:  0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 25 | \n",
      "Train BCELoss:  0.126                 | Val BCELoss:  0.519\n",
      "Train Acc:  0.995                 | Val Acc:  0.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 26 | \n",
      "Train BCELoss:  0.124                 | Val BCELoss:  0.513\n",
      "Train Acc:  0.995                 | Val Acc:  0.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 27 | \n",
      "Train BCELoss:  0.132                 | Val BCELoss:  0.508\n",
      "Train Acc:  0.989                 | Val Acc:  0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 28 | \n",
      "Train BCELoss:  0.125                 | Val BCELoss:  0.490\n",
      "Train Acc:  0.992                 | Val Acc:  0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 29 | \n",
      "Train BCELoss:  0.114                 | Val BCELoss:  0.509\n",
      "Train Acc:  0.994                 | Val Acc:  0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 30 | \n",
      "Train BCELoss:  0.106                 | Val BCELoss:  0.515\n",
      "Train Acc:  0.995                 | Val Acc:  0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 31 | \n",
      "Train BCELoss:  0.103                 | Val BCELoss:  0.497\n",
      "Train Acc:  0.994                 | Val Acc:  0.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 32 | \n",
      "Train BCELoss:  0.131                 | Val BCELoss:  0.496\n",
      "Train Acc:  0.985                 | Val Acc:  0.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 33 | \n",
      "Train BCELoss:  0.103                 | Val BCELoss:  0.488\n",
      "Train Acc:  0.994                 | Val Acc:  0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 34 | \n",
      "Train BCELoss:  0.093                 | Val BCELoss:  0.507\n",
      "Train Acc:  0.995                 | Val Acc:  0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 35 | \n",
      "Train BCELoss:  0.090                 | Val BCELoss:  0.525\n",
      "Train Acc:  0.995                 | Val Acc:  0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 36 | \n",
      "Train BCELoss:  0.101                 | Val BCELoss:  0.518\n",
      "Train Acc:  0.989                 | Val Acc:  0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 37 | \n",
      "Train BCELoss:  0.090                 | Val BCELoss:  0.547\n",
      "Train Acc:  0.993                 | Val Acc:  0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 38 | \n",
      "Train BCELoss:  0.079                 | Val BCELoss:  0.553\n",
      "Train Acc:  0.995                 | Val Acc:  0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 39 | \n",
      "Train BCELoss:  0.076                 | Val BCELoss:  0.568\n",
      "Train Acc:  0.995                 | Val Acc:  0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 40 | \n",
      "Train BCELoss:  0.076                 | Val BCELoss:  0.576\n",
      "Train Acc:  0.995                 | Val Acc:  0.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 41 | \n",
      "Train BCELoss:  0.072                 | Val BCELoss:  0.587\n",
      "Train Acc:  0.995                 | Val Acc:  0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 42 | \n",
      "Train BCELoss:  0.069                 | Val BCELoss:  0.601\n",
      "Train Acc:  0.995                 | Val Acc:  0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 43 | \n",
      "Train BCELoss:  0.066                 | Val BCELoss:  0.577\n",
      "Train Acc:  0.995                 | Val Acc:  0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 44 | \n",
      "Train BCELoss:  0.066                 | Val BCELoss:  0.593\n",
      "Train Acc:  0.995                 | Val Acc:  0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 45 | \n",
      "Train BCELoss:  0.063                 | Val BCELoss:  0.598\n",
      "Train Acc:  0.995                 | Val Acc:  0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 46 | \n",
      "Train BCELoss:  0.059                 | Val BCELoss:  0.608\n",
      "Train Acc:  0.995                 | Val Acc:  0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 47 | \n",
      "Train BCELoss:  0.057                 | Val BCELoss:  0.606\n",
      "Train Acc:  0.995                 | Val Acc:  0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 48 | \n",
      "Train BCELoss:  0.065                 | Val BCELoss:  0.603\n",
      "Train Acc:  0.992                 | Val Acc:  0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 49 | \n",
      "Train BCELoss:  0.074                 | Val BCELoss:  0.637\n",
      "Train Acc:  0.987                 | Val Acc:  0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 50 | \n",
      "Train BCELoss:  0.071                 | Val BCELoss:  0.609\n",
      "Train Acc:  0.988                 | Val Acc:  0.806\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_cnt_train = 0\n",
    "            total_loss_train = 0\n",
    "            train_acc = 0\n",
    "            train_acc_cnt = 0\n",
    "\n",
    "            for train_input, train_label, train_features in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                train_features = train_features.to(torch.float32).to(device)\n",
    "\n",
    "                output = model(input_id, mask, train_features)\n",
    "                \n",
    "                # print(\"Output1: \", output, \" Output2: \", train_label.float().unsqueeze(1), \" loss: \" , criterion(output, train_label.float()))\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.float().unsqueeze(1))\n",
    "                total_loss_train += batch_loss.item()\n",
    "                # if total_cnt_train == 5:\n",
    "                #     print(\"Train LOSS:\", batch_loss.item())\n",
    "                total_cnt_train += 1\n",
    "                \n",
    "                train_acc += get_accuracy(train_label.float().unsqueeze(1).cpu(), output.cpu())\n",
    "                train_acc_cnt += 1\n",
    " \n",
    "    \n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_cnt_val = 0\n",
    "            total_loss_val = 0\n",
    "            acc = 0\n",
    "            acc_cnt = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label, val_features in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                    val_features = val_features.to(torch.float32).to(device)\n",
    "\n",
    "                    output = model(input_id, mask, val_features)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.float().unsqueeze(1))\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    # if total_cnt_val == 5:\n",
    "                    #     print(\"Val LOSS:\", batch_loss.item())\n",
    "                    total_cnt_val += 1\n",
    "                    # print(\"Loss: \", batch_loss, \" Calc: \", sum((output - val_label.float().unsqueeze(1))**2) / batch_size)\n",
    "                    acc += get_accuracy(val_label.float().unsqueeze(1).cpu(), output.cpu())\n",
    "                    acc_cnt += 1\n",
    "\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | \\nTrain BCELoss: {total_loss_train / total_cnt_train: .3f} \\\n",
    "                | Val BCELoss: {total_loss_val / total_cnt_val: .3f}' +\n",
    "                f'\\nTrain Acc: {train_acc / train_acc_cnt: .3f} \\\n",
    "                | Val Acc: {acc / acc_cnt: .3f}'\n",
    "            )\n",
    "                  \n",
    "            writer.add_scalar('Loss/train', total_loss_train / total_cnt_train, epoch_num)\n",
    "            writer.add_scalar('Loss/val', total_loss_val / total_cnt_val, epoch_num)\n",
    "            writer.add_scalar('Acc/train', train_acc / train_acc_cnt, epoch_num)\n",
    "            writer.add_scalar('Acc/val', acc / acc_cnt, epoch_num)\n",
    "            torch.save(model.state_dict(), \"./Bert_classificationnoo_drop/BERT-CLASSIFICATION_it\" + str(epoch_num) + \".pt\")\n",
    "\n",
    "\n",
    "EPOCHS = 50\n",
    "model = BertClassifier()\n",
    "LR = 3e-5\n",
    "     \n",
    "writer = SummaryWriter()\n",
    "train(model, df_train, df_val, LR, EPOCHS)\n",
    "writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Visualization of Result <a name = \"vis\"> </a>\n",
    "\n",
    "## Purple: With Dropout; Yellow: No Dropout\n",
    "\n",
    "\n",
    "![Results](./5.1Result.png \"Results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion <a name = \"conc\"> </a>\n",
    "\n",
    "We trained the model without a Dropout Layer for 50 epochs.\n",
    "\n",
    "When comparing the results to the model that includes dropout layers, we observed that although dropout may slightly slow down the training process, it leads to nearly a **5% increase** in **test set accuracy** and a substantially lower loss compared to models without dropout.\n",
    "\n",
    "In conclusion, **integrating dropout layers greatly increased the performance of our model**. Dropout strengthens the **generalization abilities of deep learning models**, making it a crucial component in model development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
