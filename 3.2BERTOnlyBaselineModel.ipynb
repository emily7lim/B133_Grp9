{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3.2: Machine Learning - Bert-Only Model\n",
    "\n",
    "## Bert-Only Model:\n",
    "* [Train-Test Split](#train_test_split)\n",
    "* [Model Construction](#model_con)\n",
    "    * [Data Loader](#dataload)\n",
    "    * [Build Model](#build)\n",
    "    * [Train Model](#train)\n",
    "    * [Result Analysis](#ana)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hy\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, utils\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pre-trained tokenizer to transform setences into tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./distilbert-base-uncased', local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the normalized one-hot encoding dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>year</th>\n",
       "      <th>views</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>key_11</th>\n",
       "      <th>tag_country</th>\n",
       "      <th>tag_misc</th>\n",
       "      <th>tag_pop</th>\n",
       "      <th>tag_rap</th>\n",
       "      <th>tag_rb</th>\n",
       "      <th>tag_rock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AKING</td>\n",
       "      <td>2015</td>\n",
       "      <td>4.432273e-05</td>\n",
       "      <td>{}</td>\n",
       "      <td>Glorious mistakes are anxiously waiting to be ...</td>\n",
       "      <td>985583</td>\n",
       "      <td>https://open.spotify.com/track/30sr35axWFPOvmi...</td>\n",
       "      <td>0.760040</td>\n",
       "      <td>0.806517</td>\n",
       "      <td>0.144170</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Filip Winther</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.251733e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Intro]\\nDe-de-deluxe\\n\\n[Refräng]\\nJag fuckar...</td>\n",
       "      <td>5097257</td>\n",
       "      <td>https://open.spotify.com/track/4mznGf6tTvHp74y...</td>\n",
       "      <td>0.020681</td>\n",
       "      <td>0.894094</td>\n",
       "      <td>0.141797</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dan Reeder</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.513459e-05</td>\n",
       "      <td>{}</td>\n",
       "      <td>The guy who bathes in the pond at the park\\nTh...</td>\n",
       "      <td>3407076</td>\n",
       "      <td>https://open.spotify.com/track/1UbSSyqIVEkooKe...</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.554990</td>\n",
       "      <td>0.044422</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Noa Azazel</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.251733e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Pre-Chorus]\\nWhen the moon is taking over i'm...</td>\n",
       "      <td>7061926</td>\n",
       "      <td>https://open.spotify.com/track/51F8whLH1Qou7iV...</td>\n",
       "      <td>0.214858</td>\n",
       "      <td>0.419552</td>\n",
       "      <td>0.169140</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>070 Phi</td>\n",
       "      <td>2019</td>\n",
       "      <td>2.031221e-05</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Chorus]\\nAin't no way that you ain't eatin' w...</td>\n",
       "      <td>4241387</td>\n",
       "      <td>https://open.spotify.com/track/0mvzUwvyLT1Dm1y...</td>\n",
       "      <td>0.367469</td>\n",
       "      <td>0.695519</td>\n",
       "      <td>0.146753</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9529</th>\n",
       "      <td>mounika yadav</td>\n",
       "      <td>2021</td>\n",
       "      <td>1.257423e-05</td>\n",
       "      <td>{\"Allu Arjun\",\"Rashmika Mandanna\"}</td>\n",
       "      <td>నువ్ అమ్మీ అమ్మీ అంటాంటే నీ పెళ్ళాన్నైపోయినట్ట...</td>\n",
       "      <td>7552375</td>\n",
       "      <td>https://open.spotify.com/track/4ZUxhQNRCzlh6al...</td>\n",
       "      <td>0.360441</td>\n",
       "      <td>0.821792</td>\n",
       "      <td>0.161581</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9530</th>\n",
       "      <td>d-metal stars</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.706909e-07</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Verse 1]\\nThe seaweed is always greener\\nIn s...</td>\n",
       "      <td>7558599</td>\n",
       "      <td>https://open.spotify.com/track/0F8nLktPi0SgOAm...</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.542770</td>\n",
       "      <td>0.154411</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9531</th>\n",
       "      <td>grupo firme</td>\n",
       "      <td>2021</td>\n",
       "      <td>2.048290e-06</td>\n",
       "      <td>{Maluma}</td>\n",
       "      <td>Dejen de meterse ya, en donde no les importa\\n...</td>\n",
       "      <td>7728445</td>\n",
       "      <td>https://open.spotify.com/track/5BE9B2FiFWBbBdo...</td>\n",
       "      <td>0.137549</td>\n",
       "      <td>0.719959</td>\n",
       "      <td>0.142190</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9532</th>\n",
       "      <td>hensonn</td>\n",
       "      <td>2021</td>\n",
       "      <td>7.567295e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Instrumental]</td>\n",
       "      <td>7814578</td>\n",
       "      <td>https://open.spotify.com/track/6nqdgUTiWt4JbAB...</td>\n",
       "      <td>0.146585</td>\n",
       "      <td>0.626273</td>\n",
       "      <td>0.122640</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9533</th>\n",
       "      <td>ndarboy genk</td>\n",
       "      <td>2022</td>\n",
       "      <td>1.593115e-06</td>\n",
       "      <td>{}</td>\n",
       "      <td>[Intro]\\nMendung tanpo udan\\nKetemu lan kelang...</td>\n",
       "      <td>7822659</td>\n",
       "      <td>https://open.spotify.com/track/0Z54rUZ81Vn0qph...</td>\n",
       "      <td>0.332328</td>\n",
       "      <td>0.613035</td>\n",
       "      <td>0.221762</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9532 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist  year         views                            features  \\\n",
       "0             AKING  2015  4.432273e-05                                  {}   \n",
       "1     Filip Winther  2020  1.251733e-06                                  {}   \n",
       "2        Dan Reeder  2018  1.513459e-05                                  {}   \n",
       "3        Noa Azazel  2021  1.251733e-06                                  {}   \n",
       "4           070 Phi  2019  2.031221e-05                                  {}   \n",
       "...             ...   ...           ...                                 ...   \n",
       "9529  mounika yadav  2021  1.257423e-05  {\"Allu Arjun\",\"Rashmika Mandanna\"}   \n",
       "9530  d-metal stars  2016  1.706909e-07                                  {}   \n",
       "9531    grupo firme  2021  2.048290e-06                            {Maluma}   \n",
       "9532        hensonn  2021  7.567295e-06                                  {}   \n",
       "9533   ndarboy genk  2022  1.593115e-06                                  {}   \n",
       "\n",
       "                                                 lyrics       id  \\\n",
       "0     Glorious mistakes are anxiously waiting to be ...   985583   \n",
       "1     [Intro]\\nDe-de-deluxe\\n\\n[Refräng]\\nJag fuckar...  5097257   \n",
       "2     The guy who bathes in the pond at the park\\nTh...  3407076   \n",
       "3     [Pre-Chorus]\\nWhen the moon is taking over i'm...  7061926   \n",
       "4     [Chorus]\\nAin't no way that you ain't eatin' w...  4241387   \n",
       "...                                                 ...      ...   \n",
       "9529  నువ్ అమ్మీ అమ్మీ అంటాంటే నీ పెళ్ళాన్నైపోయినట్ట...  7552375   \n",
       "9530  [Verse 1]\\nThe seaweed is always greener\\nIn s...  7558599   \n",
       "9531  Dejen de meterse ya, en donde no les importa\\n...  7728445   \n",
       "9532                                     [Instrumental]  7814578   \n",
       "9533  [Intro]\\nMendung tanpo udan\\nKetemu lan kelang...  7822659   \n",
       "\n",
       "                                                    url  acousticness  \\\n",
       "0     https://open.spotify.com/track/30sr35axWFPOvmi...      0.760040   \n",
       "1     https://open.spotify.com/track/4mznGf6tTvHp74y...      0.020681   \n",
       "2     https://open.spotify.com/track/1UbSSyqIVEkooKe...      0.993976   \n",
       "3     https://open.spotify.com/track/51F8whLH1Qou7iV...      0.214858   \n",
       "4     https://open.spotify.com/track/0mvzUwvyLT1Dm1y...      0.367469   \n",
       "...                                                 ...           ...   \n",
       "9529  https://open.spotify.com/track/4ZUxhQNRCzlh6al...      0.360441   \n",
       "9530  https://open.spotify.com/track/0F8nLktPi0SgOAm...      0.000092   \n",
       "9531  https://open.spotify.com/track/5BE9B2FiFWBbBdo...      0.137549   \n",
       "9532  https://open.spotify.com/track/6nqdgUTiWt4JbAB...      0.146585   \n",
       "9533  https://open.spotify.com/track/0Z54rUZ81Vn0qph...      0.332328   \n",
       "\n",
       "      danceability  duration_ms  ...  key_8  key_9  key_10  key_11  \\\n",
       "0         0.806517     0.144170  ...      0      0       0       0   \n",
       "1         0.894094     0.141797  ...      0      0       0       1   \n",
       "2         0.554990     0.044422  ...      0      0       0       0   \n",
       "3         0.419552     0.169140  ...      0      0       0       0   \n",
       "4         0.695519     0.146753  ...      0      0       0       0   \n",
       "...            ...          ...  ...    ...    ...     ...     ...   \n",
       "9529      0.821792     0.161581  ...      0      0       0       0   \n",
       "9530      0.542770     0.154411  ...      1      0       0       0   \n",
       "9531      0.719959     0.142190  ...      0      0       0       0   \n",
       "9532      0.626273     0.122640  ...      0      0       0       0   \n",
       "9533      0.613035     0.221762  ...      0      0       0       0   \n",
       "\n",
       "      tag_country  tag_misc  tag_pop  tag_rap  tag_rb tag_rock  \n",
       "0               0         0        1        0       0        0  \n",
       "1               0         0        0        1       0        0  \n",
       "2               0         0        1        0       0        0  \n",
       "3               0         0        1        0       0        0  \n",
       "4               0         0        0        1       0        0  \n",
       "...           ...       ...      ...      ...     ...      ...  \n",
       "9529            0         0        1        0       0        0  \n",
       "9530            0         0        0        0       0        1  \n",
       "9531            0         0        1        0       0        0  \n",
       "9532            0         0        0        1       0        0  \n",
       "9533            0         0        1        0       0        0  \n",
       "\n",
       "[9532 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./positive_and_negative_one_hot.csv\")\n",
    "df = df.dropna()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split <a name = \"train_test_split\"> </a>\n",
    "\n",
    "**Train Dataset**: Used to fit the machine learning model.\n",
    "\n",
    "**Test Dataset**: Used to evaluate the fit machine learning model.\n",
    "\n",
    "The objective is to estimate the performance of the machine learning model on new data (data not used to train the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6666)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=88), \n",
    "                                     [int(.6*len(df)), int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction <a name = \"model_con\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader <a name = \"dataload\"> </a>\n",
    "\n",
    "**Get Batch Data**: Iterate through the dataset with batch size = 32.\n",
    "\n",
    "Code for processing data samples can get messy and hard to maintain, therefore, the dataset code below is used to decouple data from our model training code for better readability and modularity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.ys = df['if_popular'].to_numpy()\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['lyrics']]\n",
    "        self.df = df\n",
    "\n",
    "    def linear(self):\n",
    "        return self.ys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        #print(\"Total len:\", len(self.ys), \" getting:\", idx)\n",
    "        return self.ys[idx]\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model <a name = \"build\"> </a>\n",
    "\n",
    "**BertModel**: BERT makes use of Transformer, an attention mechanism that learns contextual relations between words in a text.\n",
    "\n",
    "**Drop out layers**: randomly dropping out nodes during training, making the model more generalized.\n",
    "\n",
    "**ReLU**: The rectified linear activation function.\n",
    "\n",
    "**BatchNorm**: take the outputs from the A hidden layer and normalize them before passing them as the input of the next hidden layer, which can stabilizing the learning process and dramatically reducing the number of training epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOnlyClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertOnlyClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('./distilbert-base-uncased', local_files_only=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(768, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output1 = self.relu(self.linear1(dropout_output))\n",
    "        linear_output1 = self.batchnorm1(linear_output1)\n",
    "        linear_output2 = self.relu(self.linear2(linear_output1))\n",
    "        linear_output2 = self.batchnorm2(linear_output2)\n",
    "        linear_output2 = self.dropout(linear_output2)\n",
    "        final_layer = self.layer_out(linear_output2)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    accuracy = accuracy_score(y_true, y_prob > 0.5)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model <a name = \"train\"> </a>\n",
    "\n",
    "**Batch Size**: It has been observed that a larger batch significant degrade in the quality of the model, and its ability to generalize. The lack of generalization ability is due to the fact that large-batch methods tend to converge to sharp minimizers of the training function. However, small batch size would be too noisy for the model to convergence fast.\n",
    "\n",
    "**Adam Optimizer**: Adam is an extension of stochastic gradient descent (SGD) that combines the benefits of two popular adaptive learning rate methods: AdaGrad and RMSProp. It requires minimal tuning of hyperparameters, and often leads to faster convergence and better performance than traditional SGD and other adaptive learning rate methods\n",
    "\n",
    "**BCEWithLogitsLoss**: BCEWithLogitsLoss is a loss function that combines the Binary Cross Entropy (BCE) Loss with a sigmoid activation function. It is designed for binary classification problems, where the goal is to distinguish between two classes, typically represented by labels 0 and 1.\n",
    "\n",
    "**SummaryWriter**: SummaryWriter is a library that allows user to log various types of data (Time Series and ) for visualization in TensorBoard. TensorBoard is a web-based visualization tool developed by Google as part of the TensorFlow ecosystem, but it can also be used with other frameworks like PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at ./distilbert-base-uncased were not used when initializing BertModel: ['distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./distilbert-base-uncased and are newly initialized: ['encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.attention.self.key.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'pooler.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'pooler.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.6.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | \n",
      "Train BCELoss:  0.569                 | Val BCELoss:  0.487\n",
      "Train Acc:  0.666                 | Val Acc:  0.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | \n",
      "Train BCELoss:  0.449                 | Val BCELoss:  0.460\n",
      "Train Acc:  0.796                 | Val Acc:  0.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | \n",
      "Train BCELoss:  0.392                 | Val BCELoss:  0.454\n",
      "Train Acc:  0.830                 | Val Acc:  0.793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | \n",
      "Train BCELoss:  0.304                 | Val BCELoss:  0.459\n",
      "Train Acc:  0.873                 | Val Acc:  0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | \n",
      "Train BCELoss:  0.219                 | Val BCELoss:  0.529\n",
      "Train Acc:  0.920                 | Val Acc:  0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | \n",
      "Train BCELoss:  0.160                 | Val BCELoss:  0.494\n",
      "Train Acc:  0.948                 | Val Acc:  0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | \n",
      "Train BCELoss:  0.133                 | Val BCELoss:  0.555\n",
      "Train Acc:  0.961                 | Val Acc:  0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | \n",
      "Train BCELoss:  0.093                 | Val BCELoss:  0.608\n",
      "Train Acc:  0.974                 | Val Acc:  0.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | \n",
      "Train BCELoss:  0.081                 | Val BCELoss:  0.669\n",
      "Train Acc:  0.981                 | Val Acc:  0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | \n",
      "Train BCELoss:  0.083                 | Val BCELoss:  0.605\n",
      "Train Acc:  0.976                 | Val Acc:  0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | \n",
      "Train BCELoss:  0.067                 | Val BCELoss:  0.599\n",
      "Train Acc:  0.987                 | Val Acc:  0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | \n",
      "Train BCELoss:  0.056                 | Val BCELoss:  0.638\n",
      "Train Acc:  0.988                 | Val Acc:  0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | \n",
      "Train BCELoss:  0.051                 | Val BCELoss:  0.740\n",
      "Train Acc:  0.990                 | Val Acc:  0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | \n",
      "Train BCELoss:  0.048                 | Val BCELoss:  0.719\n",
      "Train Acc:  0.991                 | Val Acc:  0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | \n",
      "Train BCELoss:  0.046                 | Val BCELoss:  0.683\n",
      "Train Acc:  0.990                 | Val Acc:  0.822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 16 | \n",
      "Train BCELoss:  0.148                 | Val BCELoss:  0.551\n",
      "Train Acc:  0.933                 | Val Acc:  0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 17 | \n",
      "Train BCELoss:  0.092                 | Val BCELoss:  0.696\n",
      "Train Acc:  0.967                 | Val Acc:  0.820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 18 | \n",
      "Train BCELoss:  0.051                 | Val BCELoss:  0.771\n",
      "Train Acc:  0.988                 | Val Acc:  0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 19 | \n",
      "Train BCELoss:  0.050                 | Val BCELoss:  0.779\n",
      "Train Acc:  0.990                 | Val Acc:  0.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 20 | \n",
      "Train BCELoss:  0.038                 | Val BCELoss:  0.792\n",
      "Train Acc:  0.993                 | Val Acc:  0.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 21 | \n",
      "Train BCELoss:  0.038                 | Val BCELoss:  0.820\n",
      "Train Acc:  0.993                 | Val Acc:  0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 22 | \n",
      "Train BCELoss:  0.031                 | Val BCELoss:  0.859\n",
      "Train Acc:  0.995                 | Val Acc:  0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 23 | \n",
      "Train BCELoss:  0.040                 | Val BCELoss:  0.787\n",
      "Train Acc:  0.992                 | Val Acc:  0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 24 | \n",
      "Train BCELoss:  0.040                 | Val BCELoss:  0.909\n",
      "Train Acc:  0.992                 | Val Acc:  0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 25 | \n",
      "Train BCELoss:  0.039                 | Val BCELoss:  0.948\n",
      "Train Acc:  0.991                 | Val Acc:  0.734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 26 | \n",
      "Train BCELoss:  0.035                 | Val BCELoss:  0.887\n",
      "Train Acc:  0.993                 | Val Acc:  0.807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 27 | \n",
      "Train BCELoss:  0.027                 | Val BCELoss:  0.905\n",
      "Train Acc:  0.995                 | Val Acc:  0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 28 | \n",
      "Train BCELoss:  0.034                 | Val BCELoss:  0.944\n",
      "Train Acc:  0.993                 | Val Acc:  0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 29 | \n",
      "Train BCELoss:  0.050                 | Val BCELoss:  0.852\n",
      "Train Acc:  0.988                 | Val Acc:  0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 30 | \n",
      "Train BCELoss:  0.042                 | Val BCELoss:  0.728\n",
      "Train Acc:  0.991                 | Val Acc:  0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 31 | \n",
      "Train BCELoss:  0.036                 | Val BCELoss:  0.860\n",
      "Train Acc:  0.992                 | Val Acc:  0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 32 | \n",
      "Train BCELoss:  0.028                 | Val BCELoss:  0.848\n",
      "Train Acc:  0.995                 | Val Acc:  0.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 33 | \n",
      "Train BCELoss:  0.034                 | Val BCELoss:  0.968\n",
      "Train Acc:  0.991                 | Val Acc:  0.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 34 | \n",
      "Train BCELoss:  0.028                 | Val BCELoss:  0.957\n",
      "Train Acc:  0.994                 | Val Acc:  0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 35 | \n",
      "Train BCELoss:  0.025                 | Val BCELoss:  0.982\n",
      "Train Acc:  0.995                 | Val Acc:  0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 36 | \n",
      "Train BCELoss:  0.024                 | Val BCELoss:  0.982\n",
      "Train Acc:  0.995                 | Val Acc:  0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 37 | \n",
      "Train BCELoss:  0.024                 | Val BCELoss:  0.994\n",
      "Train Acc:  0.995                 | Val Acc:  0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 38 | \n",
      "Train BCELoss:  0.026                 | Val BCELoss:  0.992\n",
      "Train Acc:  0.995                 | Val Acc:  0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 39 | \n",
      "Train BCELoss:  0.025                 | Val BCELoss:  1.006\n",
      "Train Acc:  0.995                 | Val Acc:  0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 179/179 [01:26<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 40 | \n",
      "Train BCELoss:  0.027                 | Val BCELoss:  0.999\n",
      "Train Acc:  0.995                 | Val Acc:  0.809\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_cnt_train = 0\n",
    "            total_loss_train = 0\n",
    "            train_acc = 0\n",
    "            train_acc_cnt = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                # print(\"Output1: \", output, \" Output2: \", train_label.float().unsqueeze(1), \" loss: \" , criterion(output, train_label.float()))\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.float().unsqueeze(1))\n",
    "                total_loss_train += batch_loss.item()\n",
    "                # if total_cnt_train == 5:\n",
    "                #     print(\"Train LOSS:\", batch_loss.item())\n",
    "                total_cnt_train += 1\n",
    "                \n",
    "                train_acc += get_accuracy(train_label.float().unsqueeze(1).cpu(), output.cpu())\n",
    "                train_acc_cnt += 1\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_cnt_val = 0\n",
    "            total_loss_val = 0\n",
    "            acc = 0\n",
    "            acc_cnt = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.float().unsqueeze(1))\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    # if total_cnt_val == 5:\n",
    "                    #     print(\"Val LOSS:\", batch_loss.item())\n",
    "                    total_cnt_val += 1\n",
    "                    # print(\"Loss: \", batch_loss, \" Calc: \", sum((output - val_label.float().unsqueeze(1))**2) / batch_size)\n",
    "                    acc += get_accuracy(val_label.float().unsqueeze(1).cpu(), output.cpu())\n",
    "                    acc_cnt += 1\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | \\nTrain BCELoss: {total_loss_train / total_cnt_train: .3f} \\\n",
    "                | Val BCELoss: {total_loss_val / total_cnt_val: .3f}' +\n",
    "                f'\\nTrain Acc: {train_acc / train_acc_cnt: .3f} \\\n",
    "                | Val Acc: {acc / acc_cnt: .3f}'\n",
    "            )\n",
    "                  \n",
    "            writer.add_scalar('Loss/train', total_loss_train / total_cnt_train, epoch_num)\n",
    "            writer.add_scalar('Loss/val', total_loss_val / total_cnt_val, epoch_num)\n",
    "            writer.add_scalar('Acc/train', train_acc / train_acc_cnt, epoch_num)\n",
    "            writer.add_scalar('Acc/val', acc / acc_cnt, epoch_num)\n",
    "            torch.save(model.state_dict(), \"./Bert_only_classification/BERT-ONLY-CLASSIFICATION_it\" + str(epoch_num) + \".pt\")\n",
    "\n",
    "\n",
    "EPOCHS = 40\n",
    "model = BertOnlyClassifier()\n",
    "LR = 3e-5\n",
    "     \n",
    "writer = SummaryWriter()\n",
    "train(model, df_train, df_val, LR, EPOCHS)\n",
    "writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis <a name = \"ana\"> </a>\n",
    "\n",
    "**Accuracy**: Unexpectedly, using only lyrics to predict a song's popularity resulted in an impressive accuracy of nearly 80% on the test set, which is much higher than that of using Support Vector Machines (SVM) and other models. This underscores the significant role lyrics play in determining a song's popularity.\n",
    "\n",
    "**Early Stopping**: Early stopping is crucial, as demonstrated by the data above. If the number of iterations becomes excessive, the test set's loss starts to increase due to overfitting. In our specific case, halting at the 15th iteration led to a more reliable and accurate model.\n",
    "\n",
    "![Results32](./3.2Result.png \"Results32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
